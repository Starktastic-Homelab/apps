# =============================================================================
# kube-prometheus-stack Helm values
# Deploys: Prometheus, Grafana, AlertManager, node-exporter, kube-state-metrics
# =============================================================================

# -----------------------------------------------------------------------------
# K3s compatibility — disable components that don't expose standard endpoints
# -----------------------------------------------------------------------------
kubeEtcd:
  enabled: false
kubeScheduler:
  enabled: false
kubeControllerManager:
  enabled: false
kubeProxy:
  enabled: false

# -----------------------------------------------------------------------------
# Prometheus
# -----------------------------------------------------------------------------
prometheus:
  prometheusSpec:
    retention: 15d
    retentionSize: "8GB"
    # Scrape all ServiceMonitors/PodMonitors cluster-wide, not just those
    # matching the Helm release labels
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    ruleSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: nfs-pv
          accessModes: ["ReadWriteMany"]
          resources:
            requests:
              storage: 10Gi

# -----------------------------------------------------------------------------
# AlertManager
# -----------------------------------------------------------------------------
alertmanager:
  enabled: true
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: nfs-pv
          accessModes: ["ReadWriteMany"]
          resources:
            requests:
              storage: 2Gi
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ["alertname", "namespace"]
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: "ntfy"
      routes:
        # Silence Watchdog — it fires continuously as a dead-man's switch
        - receiver: "null"
          matchers:
            - alertname = "Watchdog"
        - receiver: "null"
          matchers:
            - alertname = "InfoInhibitor"
        - receiver: "ntfy"
          continue: true
    receivers:
      - name: "null"
      - name: "ntfy"
        webhook_configs:
          # Routes through alertmanager-ntfy adapter for formatted notifications
          # with proper titles, descriptions, priority, and tags
          - url: "http://alertmanager-ntfy.monitoring.svc.cluster.local:8000/hook"
            send_resolved: true
    inhibit_rules:
      - source_matchers:
          - severity = "critical"
        target_matchers:
          - severity = "warning"
        equal: ["alertname", "namespace"]

# -----------------------------------------------------------------------------
# Grafana
# -----------------------------------------------------------------------------
grafana:
  enabled: true
  admin:
    existingSecret: grafana-admin-secret
    userKey: admin-user
    passwordKey: admin-password
  envFromSecret: grafana-oidc-secret
  defaultDashboardsTimezone: Asia/Jerusalem
  initChownData:
    enabled: false
  podSecurityContext:
    fsGroup: 472
    runAsUser: 472
    runAsGroup: 472
  grafana.ini:
    server:
      root_url: https://grafana.internal.starktastic.net
    auth.anonymous:
      enabled: false
    auth.generic_oauth:
      enabled: true
      name: Authentik
      allow_sign_up: true
      auto_login: true
      scopes: openid profile email
      auth_url: https://auth.starktastic.net/application/o/authorize/
      token_url: https://auth.starktastic.net/application/o/token/
      api_url: https://auth.starktastic.net/application/o/userinfo/
      # client_id & client_secret injected via GF_AUTH_GENERIC_OAUTH_CLIENT_ID
      # and GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET from grafana-oidc-secret
      role_attribute_path:
        "contains(groups[*], 'Grafana Admins') && 'Admin' || contains(groups[*], 'Grafana Editors') && 'Editor' ||
        'Viewer'"
  persistence:
    enabled: true
    storageClassName: nfs-pv
    accessModes: ["ReadWriteMany"]
    size: 2Gi
  additionalDataSources:
    - name: Loki
      type: loki
      uid: loki
      access: proxy
      url: http://loki-gateway.monitoring.svc.cluster.local
      jsonData:
        maxLines: 250
    - name: Tempo
      type: tempo
      uid: tempo
      access: proxy
      url: http://tempo.monitoring.svc.cluster.local:3200
      jsonData:
        tracesToLogsV2:
          datasourceUid: loki
          filterByTraceID: true
        nodeGraph:
          enabled: true
        serviceMap:
          datasourceUid: prometheus

# -----------------------------------------------------------------------------
# node-exporter (DaemonSet — hardware & OS metrics from every node)
# -----------------------------------------------------------------------------
nodeExporter:
  enabled: true

# -----------------------------------------------------------------------------
# kube-state-metrics (K8s object state — pod status, deployments, etc.)
# -----------------------------------------------------------------------------
kubeStateMetrics:
  enabled: true
